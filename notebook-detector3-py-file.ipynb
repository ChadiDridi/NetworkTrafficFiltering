{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7285844,"sourceType":"datasetVersion","datasetId":3843877}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Import data","metadata":{}},{"cell_type":"markdown","source":"### Citations\n#### Dataset\n“Stratosphere Laboratory. A labeled dataset with malicious and benign IoT network traffic. January 22th. Parmisano, Sebastian Garcia, Maria Jose Erquiaga. https://www.stratosphereips.org/datasets-iot23\n\n","metadata":{}},{"cell_type":"code","source":"def find_csv_delimiter(file_path, max_lines=5):\n    with open(file_path, 'r', newline='') as file:\n        sample_lines = [file.readline().strip() for _ in range(max_lines)]\n\n    delimiters = [',', ';', '\\t', '|']  # Common delimiters to check\n\n    best_delimiter = ','\n    max_delimiter_count = 0\n\n    for delimiter in delimiters:\n        delimiter_count = sum(line.count(delimiter) for line in sample_lines)\n        if delimiter_count > max_delimiter_count:\n            best_delimiter = delimiter\n            max_delimiter_count = delimiter_count\n\n    return best_delimiter\n\nimport csv\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\ndef extract_data_from_csv(file_path, delimiter=','):\n    data = []  # Create a list to store the data\n\n    # Open the CSV file for reading\n    with open(file_path, mode='r', newline='') as file:\n        # Create a CSV reader object with the pipe delimiter\n\n        csv_reader = csv.reader(file, delimiter=delimiter)\n\n        # Read the header row\n        header = next(csv_reader)\n\n        # Iterate through the rows in the CSV file\n        for row in csv_reader:\n            data.append(row)\n    \n    return data\n\ndata = []\n\nimport os\ncpt = 0\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    \n        for filename in filenames:\n            if cpt < 2:\n                file_path = os.path.join(dirname, filename)\n                delimiter = find_csv_delimiter(file_path)\n                data += extract_data_from_csv(file_path, delimiter)\n                print(os.path.join(dirname, filename))\n                cpt+=1\n            else:\n                break\n        \n\n# Convert your data to a NumPy array\ndata = np.array(data)\n\nprint(data.shape)","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:42:35.466897Z","iopub.execute_input":"2025-05-30T12:42:35.467460Z","iopub.status.idle":"2025-05-30T12:43:23.503405Z","shell.execute_reply.started":"2025-05-30T12:42:35.467417Z","shell.execute_reply":"2025-05-30T12:43:23.501494Z"},"trusted":true},"outputs":[{"name":"stdout","text":"/kaggle/input/network-malware-detection-connection-analysis/CTU-IoT-Malware-Capture-8-1conn.log.labeled.csv\n/kaggle/input/network-malware-detection-connection-analysis/CTU-IoT-Malware-Capture-60-1conn.log.labeled.csv\n(3591431, 23)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"print(data[0])","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:43:23.505628Z","iopub.execute_input":"2025-05-30T12:43:23.506133Z","iopub.status.idle":"2025-05-30T12:43:23.514604Z","shell.execute_reply.started":"2025-05-30T12:43:23.506079Z","shell.execute_reply":"2025-05-30T12:43:23.512217Z"},"trusted":true},"outputs":[{"name":"stdout","text":"['1533042911.474174' 'C5JLGOoxIw2dBZt47' '192.168.100.113' '123'\n '81.2.254.224' '123' 'udp' '-' '0.005490' '48' '48' 'SF' '-' '-' '0' 'Dd'\n '1' '76' '1' '76' '-' 'Benign' '-']\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Process data","metadata":{}},{"cell_type":"code","source":"columns_to_remove = [0, 1, 2, 4, 12, 13, 14, 20, 22]\n\n# Remove columns using NumPy's array slicing\ndata = np.delete(data, columns_to_remove, axis=1)\n\n# Make Malicious = 1 and Benign = 0\nfor row in data:\n    if row[-1] == 'Benign':\n        row[-1] = 0\n    else:\n        row[-1] = 1\n\ncolumns_to_transform = [3, 4, 5, 6, 8]\n# Columns to put 0 if '-'\nzeros = [4, 5, 6]\n# Replace '-'\nfor row in data:\n    for column in columns_to_transform:\n        if row[column] == '-' and column in zeros:\n            row[column] = 0\n        elif row[column] == '-' and column not in zeros:\n            row[column] = 'Unkown'\n\ncolumns_to_convert_to_float = [4]\n# Convert columns to float\nfor row in data:\n    for column in columns_to_convert_to_float:\n        row[column] = float(row[column])\n\ncolumns_to_convert_to_int = [0, 1, 5, 6, 8, 9, 10, 11]\n\n# Convert columns to int\nfor row in data:\n    for column in columns_to_convert_to_int:\n        try:\n            # Attempt to convert the value to an integer\n            row[column] = int(row[column])\n        except (ValueError, TypeError):\n            pass\n\n# Remove rows where first column contains ip address\nrows_to_remove = []\nfor index, row in enumerate(data):\n    # If the first column is an IP address\n    if row[0].count('.') == 3:\n        rows_to_remove.append(index)\n\n# Delete the rows by index\nfor index in sorted(rows_to_remove, reverse=True):\n    del data[index]","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:43:23.517405Z","iopub.execute_input":"2025-05-30T12:43:23.517990Z","iopub.status.idle":"2025-05-30T12:44:19.808431Z","shell.execute_reply.started":"2025-05-30T12:43:23.517949Z","shell.execute_reply":"2025-05-30T12:44:19.806811Z"},"trusted":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.preprocessing import OneHotEncoder\n\n# columns_to_onehot = [0, 1, 2, 3, 7, 8]\ncolumns_to_onehot = [2, 3, 7, 8]\n\nprint('Before one-hot encoding features:')\nprint(data[0])\nprint(data[0].shape)\n\nonehot_encoder = OneHotEncoder(sparse_output=True)\n\ndataCopy = data.copy()\n\naddedCols = 0\nfor column in columns_to_onehot:\n    column_values = data[:, column]\n    onehot_encoded = onehot_encoder.fit_transform(column_values.reshape(-1, 1)).toarray()\n    dataCopy = np.delete(dataCopy, column + addedCols, axis=1)\n    \n    # Insert the new columns\n    for i, encoded_column in enumerate(onehot_encoded.T):\n        dataCopy = np.insert(dataCopy, column + i + addedCols, encoded_column, axis=1)\n\n    addedCols += onehot_encoded.shape[1] - 1\n\ndata = dataCopy\n\n\nprint('After one-hot encoding features:')\nprint(data[0])\nprint(data[0].shape)\n    \n","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:44:19.811895Z","iopub.execute_input":"2025-05-30T12:44:19.812353Z","iopub.status.idle":"2025-05-30T12:48:19.351761Z","shell.execute_reply.started":"2025-05-30T12:44:19.812316Z","shell.execute_reply":"2025-05-30T12:48:19.348291Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Before one-hot encoding features:\n['123' '123' 'udp' 'Unkown' '0.00549' '48' '48' 'SF' 'Dd' '1' '76' '1'\n '76' '0']\n(14,)\nAfter one-hot encoding features:\n['123' '123' '0.0' '0.0' '1.0' '1.0' '0.0' '0.00549' '48' '48' '0.0' '0.0'\n '0.0' '0.0' '0.0' '1.0' '0.0' '0.0' '1.0' '0.0' '0.0' '0.0' '0.0' '0.0'\n '0.0' '0.0' '0.0' '1' '76' '1' '76' '0']\n(32,)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Check if any of the data contains strings\nfor row in data:\n    for column in row:\n        if isinstance(column, str):\n            #Convert the value to a float, if possible\n            try:\n                column = float(column)\n            except ValueError:\n                pass\n\n# Initialize an empty list to store preprocessed data\npreprocessed_data = []\n\n# Iterate through the rows in the data\nfor row in data:\n    try:\n        # Convert all columns to floats in this row\n        float_row = [float(column) if column != '-' else 0.0 for column in row]\n        preprocessed_data.append(float_row)\n    except ValueError:\n        print('Skipping row with non-convertible values:', row)\n\ndata = preprocessed_data\n\n# Check if data contains strings\nfor row in data:\n    for column in row:\n        if isinstance(column, str):\n            print('Error: String found in data: ', column)\n            break\n\npreprocessed_data = data\n\n# Convert preprocessed_data to a normal Python list of lists\npreprocessed_data = [list(row) for row in preprocessed_data]\n\n# # Print the preprocessed data\n# for row in preprocessed_data:\n#     print(row)\n","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:48:19.355839Z","iopub.execute_input":"2025-05-30T12:48:19.356530Z","iopub.status.idle":"2025-05-30T12:51:30.578286Z","shell.execute_reply.started":"2025-05-30T12:48:19.356481Z","shell.execute_reply":"2025-05-30T12:51:30.576994Z"},"trusted":true},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"### Separate data","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\n# Separate data\n\n# Define the split ratios for training, validation, and test datasets\ntrain_ratio = 0.70  # 70% for training\nval_ratio = 0.15   # 15% for validation\ntest_ratio = 0.15  # 15% for testing\n\ntrain_val_indices = int((train_ratio + val_ratio) * len(preprocessed_data))\n\ntrain_val_data = preprocessed_data[:train_val_indices]\ntest_data = preprocessed_data[train_val_indices:]\n\n# Shuffle the data randomly\nrandom.shuffle(train_val_data)\nrandom.shuffle(test_data)\n\n# Calculate the split points\ntotal_records = len(train_val_data)\ntrain_split = int(train_ratio * total_records)\nval_split = int(val_ratio * total_records)\n\n# Split the data into training, validation\ntrain_data = train_val_data[:train_split]\nval_data = train_val_data[train_split:]","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:51:30.580048Z","iopub.execute_input":"2025-05-30T12:51:30.580548Z","iopub.status.idle":"2025-05-30T12:51:34.635333Z","shell.execute_reply.started":"2025-05-30T12:51:30.580509Z","shell.execute_reply":"2025-05-30T12:51:34.633948Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import glob\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pandas.plotting import scatter_matrix\nfrom collections import Counter\n\n# Parameters\ndir_path = \"/kaggle/input/network-malware-detection-connection-analysis/\"\nbatch_size = 100_000  # rows per chunk\n\n# Define bins for numeric histograms\ndurations = np.linspace(0, 1000, 50)  # adjust to your data range\nbytes_bins = np.linspace(0, 1e6, 50)\n\n# Initialize accumulators\nhist_counts = {\n    'duration': np.zeros(len(durations)-1, dtype=int),\n    'orig_bytes': np.zeros(len(bytes_bins)-1, dtype=int),\n    'resp_bytes': np.zeros(len(bytes_bins)-1, dtype=int)\n}\ncat_counters = {col: Counter() for col in ['proto', 'service', 'conn_state']}\nlabel_counter = Counter()\nsample_rows = []  # for scatter matrix sampling\n\n# Process each file in chunks\ncsv_files = glob.glob(dir_path + \"/*.csv\")\nfor path in csv_files:\n    for chunk in pd.read_csv(path, sep=\"|\", low_memory=False, chunksize=batch_size):\n        # Preprocess\n        chunk.replace('-', np.nan, inplace=True)\n        for col in ['duration', 'orig_bytes', 'resp_bytes']:\n            chunk[col] = pd.to_numeric(chunk[col], errors='coerce')\n        chunk = chunk.dropna(subset=['proto','service','duration','orig_bytes','resp_bytes','conn_state','label'])\n        chunk['label_bin'] = (chunk['label'] != 'Benign').astype(int)\n\n        # Accumulate label counts\n        label_counter.update(chunk['label'])\n\n        # Numeric histograms accumulator\n        hist_counts['duration'] += np.histogram(chunk['duration'], bins=durations)[0]\n        hist_counts['orig_bytes'] += np.histogram(chunk['orig_bytes'], bins=bytes_bins)[0]\n        hist_counts['resp_bytes'] += np.histogram(chunk['resp_bytes'], bins=bytes_bins)[0]\n\n        # Categorical counts\n        for col in ['proto', 'service', 'conn_state']:\n            cat_counters[col].update(chunk[col])\n\n        # Sample rows for scatter plot (e.g., 5% random)\n        sample_rows.append(chunk.sample(frac=0.05, random_state=42))\n\n# Combine samples\nsample_df = pd.concat(sample_rows, ignore_index=True)\n\n# --- Label Distribution Pie Chart ---\nlabels, sizes = zip(*label_counter.items())\nplt.figure(figsize=(6,6))\nplt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90)\nplt.title('Label Distribution')\nplt.axis('equal')  # Equal aspect ratio ensures pie is circular\nplt.show()\n\n# Plot numeric histograms\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\naxes[0].bar((durations[:-1] + durations[1:]) / 2, hist_counts['duration'], width=np.diff(durations))\naxes[0].set_title('Duration Histogram (batches)')\naxes[1].bar((bytes_bins[:-1] + bytes_bins[1:]) / 2, hist_counts['orig_bytes'], width=np.diff(bytes_bins))\naxes[1].set_title('Orig Bytes Histogram (batches)')\naxes[2].bar((bytes_bins[:-1] + bytes_bins[1:]) / 2, hist_counts['resp_bytes'], width=np.diff(bytes_bins))\naxes[2].set_title('Resp Bytes Histogram (batches)')\nfor ax in axes:\n    ax.set_xlabel('Value')\n    ax.set_ylabel('Count')\nplt.tight_layout()\nplt.show()\n\n# Plot categorical counts\ndef plot_counter(counter, title, ax):\n    items = counter.most_common(20)\n    keys, vals = zip(*items)\n    ax.bar(keys, vals)\n    ax.set_title(title)\n    ax.set_xticklabels(keys, rotation=45, ha='right')\n\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\nplot_counter(cat_counters['proto'], 'Proto Distribution', axes[0])\nplot_counter(cat_counters['service'], 'Service Distribution', axes[1])\nplot_counter(cat_counters['conn_state'], 'Conn State Distribution', axes[2])\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# %%\nimport glob\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OrdinalEncoder\nfrom xgboost import XGBClassifier\nimport joblib\n\n# 1) Read all CSVs with correct delimiter\ncsv_files = glob.glob(\"/kaggle/input/network-malware-detection-connection-analysis/*.csv\")\ndf_list = [pd.read_csv(path, sep=\"|\", low_memory=False) for path in csv_files]\ndf = pd.concat(df_list, ignore_index=True)\n\n# 2) Inspect columns\nprint(\"Columns in raw CSV:\", df.columns.tolist())\n\n# 3) Select the six unified features + label\nFEATURE_ORDER = ['proto','service','duration','orig_bytes','resp_bytes','conn_state']\nTARGET        = 'label'\ndf_model4     = df[FEATURE_ORDER + [TARGET]].copy()\n\n# 4a) Clean numeric columns: replace '-' with NaN, then cast\ndf_model4.replace('-', np.nan, inplace=True)\nfor col in ['duration','orig_bytes','resp_bytes']:\n    df_model4[col] = pd.to_numeric(df_model4[col], errors='coerce')\n\n# 4b) Map string labels to 0/1\ndf_model4[TARGET] = (df_model4[TARGET] != 'Benign').astype(int)\n\n# 4c) Drop any rows with missing\ndf_model4.dropna(subset=FEATURE_ORDER + [TARGET], inplace=True)\nprint(\"After cleaning + dropna:\", df_model4.shape)\nprint(df_model4[TARGET].value_counts())\n\n# 5) Train/validation split\nX_train, X_val, y_train, y_val = train_test_split(\n    df_model4[FEATURE_ORDER],\n    df_model4[TARGET],\n    test_size=0.15,\n    random_state=42,\n    stratify=df_model4[TARGET]\n)\n\n# 6) Build pipeline: encode & scale\npreprocessor = ColumnTransformer([\n    (\"ord\",   OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1),\n               ['proto','service','conn_state']),\n    (\"scale\", StandardScaler(), ['duration','orig_bytes','resp_bytes'])\n])\n\npipeline = Pipeline([\n    (\"preprocessor\", preprocessor),\n    (\"classifier\",   XGBClassifier(\n         use_label_encoder=False, eval_metric=\"logloss\", random_state=42\n    ))\n])\n\n# 7) Fit & save\npipeline.fit(X_train, y_train)\njoblib.dump(pipeline, \"model_4_pipeline_ordinal.pkl\")\nprint(\"model_4_pipeline_ordinal.pkl saved.\")\n\n# 8) Sanity‐check\nfrom sklearn.metrics import accuracy_score\ny_pred = pipeline.predict(X_val)\nprint(\"Validation accuracy:\", accuracy_score(y_val, y_pred))\n","metadata":{"execution":{"iopub.status.busy":"2025-05-30T12:51:34.637128Z","iopub.execute_input":"2025-05-30T12:51:34.637510Z","execution_failed":"2025-05-30T12:55:24.768Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Columns in raw CSV: ['ts', 'uid', 'id.orig_h', 'id.orig_p', 'id.resp_h', 'id.resp_p', 'proto', 'service', 'duration', 'orig_bytes', 'resp_bytes', 'conn_state', 'local_orig', 'local_resp', 'missed_bytes', 'history', 'orig_pkts', 'orig_ip_bytes', 'resp_pkts', 'resp_ip_bytes', 'tunnel_parents', 'label', 'detailed-label']\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nimport numpy as np\n\n# Define your XGBoost classifier and hyperparameter search space\nxgb_model = XGBClassifier()\nparam_space = {\n    'n_estimators': [100],\n    'max_depth': [3, 4, 5, 6],\n    'learning_rate': [0.01, 0.1, 0.2, 0.3],\n    'subsample': [0.8, 0.9, 1.0],\n    'colsample_bytree': [0.8, 0.9, 1.0],\n    'gamma': [0, 0.1, 0.2, 0.3],\n}\n\n# Create a RandomizedSearchCV object\nrandom_search = RandomizedSearchCV(\n    xgb_model,\n    param_space,\n    n_iter=5,  # Adjust the number of iterations as needed\n    scoring='accuracy',  # Use the appropriate scoring metric\n    n_jobs=-1,  # Use all available CPU cores for parallel processing\n    cv=5,  # Number of cross-validation folds\n    random_state=42,  # Set a random seed for reproducibility\n    verbose=3\n)\n\n# Perform hyperparameter optimization\nrandom_search.fit(train_data, train_labels)\n\n# Get the best hyperparameters and the best model\nbest_xgb_hps = random_search.best_params_\nbest_xgb_model = random_search.best_estimator_\n","metadata":{"execution":{"execution_failed":"2025-05-30T12:55:24.770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(best_xgb_hps)","metadata":{"execution":{"execution_failed":"2025-05-30T12:55:24.770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate model on val set","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report, confusion_matrix\n# Evaluate the Random Forest model on the validation data\nsgboost_val_predictions = random_search.predict(val_data)\nsgboost_val_accuracy = np.mean(sgboost_val_predictions == val_labels)\nprint(\"Validation Accuracy (Random Forest):\", sgboost_val_accuracy)\n\n# Calculate and print classification report and confusion matrix for Random Forest\nsgboost_val_report = classification_report(val_labels, sgboost_val_predictions)\nsgboost_val_confusion = confusion_matrix(val_labels, sgboost_val_predictions)\nprint(\"Validation Classification Report (XGBoost):\")\nprint(sgboost_val_report)\nprint(\"Validation Confusion Matrix (XGBoost):\")\nprint(sgboost_val_confusion)\n","metadata":{"execution":{"execution_failed":"2025-05-30T12:55:24.770Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Evaluate model on test set","metadata":{}},{"cell_type":"code","source":"# Evaluate the Random Forest model on the test data\nsgboost_test_predictions = random_search.predict(test_data)\nsgboost_test_accuracy = np.mean(sgboost_test_predictions == test_labels)\nprint(\"Test Accuracy (SGBoost):\", sgboost_test_accuracy)\n\n# Calculate and print classification report and confusion matrix for Random Forest\nsgboost_test_report = classification_report(test_labels, sgboost_test_predictions)\nsgboost_test_confusion = confusion_matrix(test_labels, sgboost_test_predictions)\nprint(\"Test Classification Report (XGBoost):\")\nprint(sgboost_test_report)\nprint(\"Test Confusion Matrix (XGBoost):\")\nprint(sgboost_test_confusion)","metadata":{"execution":{"execution_failed":"2025-05-30T12:55:24.771Z"},"trusted":true},"outputs":[],"execution_count":null}]}